# guitar-to-midi
Uses machine learning to improve the generation of MIDI from guitar playing.

# Motivation
Technology already exists to generate a MIDI sequence from a guitar recording.  However, the generated sequence is not always a faithful representation of the original guitar playing.  Here I propose a method utilizing machine learning to improve such a representation.

# Method
To summarize my proposed method, it is best to simply run through an example of the process.  The first example I will run through is in fact not the ultimate method I am proposing, but it could also be a useful tool, and explicating it will serve as a good foundation for the ultimate method I am proposing.
1. Record a guitarist playing through a song.  Let's call this audio file "human.wav".
2. Use pre-existing technology to generate a MIDI sequence from the above recording.  Let's call this "midi01.mid"
3. Use a pre-existing synthesizer (with an appropriate digital instrument chosen, like "nylon guitar") to generate an audio file from the above MIDI sequence.  Let's call this audio file "computer.wav".
4. Create some kind of distance measure between human.wav and computer.wav and compute such a distance.  Let's call the computed distance "error".
5. Modify the MIDI sequence in some way that reduces the above error, creating "midi02.mid", and then repeat steps 3-5 until the error is low enough.
Now, the MIDI sequence, and the resulting WAV file that can be generated by it, should be a fairly faithful representation of the original human's recording.  Some advantages of having such a faithful representation would be seeing if a piece of music originally composed for and recorded by a classical guitarist would sound cool being played by an electric guitarist or some cool synth sound (or a virtual choir... possibilities are endless, obviously).

Now of course the step that was the most "hand wavy" was step five: how should we modify the MIDI sequence?  But instead of trying to fill in the gaps in the above vague sketch of an algorithm, I'll proceed to attempt to articulate the ultimate method I'm proposing, via an example.
1. Record a guitarist playing through a song.  Let's call this audio file "human.wav".
2. Use some neural network to generate a MIDI sequence from the above recording.  Let's call this "midi01.mid".
3. Use a pre-existing synthesizer (with an appropriate digital instrument chosen, like "nylon guitar") to generate an audio file from the above MIDI sequence.  Let's call this audio file "computer.wav".
4. Create some kind of distance measure between human.wav and computer.wav and compute such a distance.  Let's call the computed distance "error".  Choose an appropriate loss function that takes this error as an input.
5. Modify the weights in the neural network to minimize the loss.  Generate a new MIDI sequence from the modified neural network, creating "midi02.mid", and then repeat steps 3-5 until the neural network converges.
What kind of neural network will generate a MIDI sequence from a WAV file?  Not sure, but that's where the fun will be!
